


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrec.distributed.sharding &mdash; TorchRec 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torchrec.fx" href="torchrec.fx.html" />
    <link rel="prev" title="torchrec.distributed.planner" href="torchrec.distributed.planner.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torchrec.datasets.html">torchrec.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.datasets.scripts.html">torchrec.datasets.scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.distributed.html">torchrec.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.distributed.planner.html">torchrec.distributed.planner</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torchrec.distributed.sharding</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.fx.html">torchrec.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.inference.html">torchrec.inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.models.html">torchrec.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.modules.html">torchrec.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.optim.html">torchrec.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.quant.html">torchrec.quant</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrec.sparse.html">torchrec.sparse</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torchrec.distributed.sharding</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/torchrec.distributed.sharding.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="module-torchrec.distributed.sharding">
<span id="torchrec-distributed-sharding"></span><h1>torchrec.distributed.sharding<a class="headerlink" href="#module-torchrec.distributed.sharding" title="Permalink to this heading">Â¶</a></h1>
<section id="module-torchrec.distributed.sharding.cw_sharding">
<span id="torchrec-distributed-sharding-cw-sharding"></span><h2>torchrec.distributed.sharding.cw_sharding<a class="headerlink" href="#module-torchrec.distributed.sharding.cw_sharding" title="Permalink to this heading">Â¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.cw_sharding.</span></span><span class="sig-name descname"><span class="pre">BaseCwEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">permute_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding" title="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding</span></code></a>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">C</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">F</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">W</span></code>]</p>
<p>Base class for column-wise sharding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_dims">
<span class="sig-name descname"><span class="pre">embedding_dims</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_dims" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_names">
<span class="sig-name descname"><span class="pre">embedding_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.cw_sharding.</span></span><span class="sig-name descname"><span class="pre">CwPooledEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">permute_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding" title="torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseCwEmbeddingSharding</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Shards embedding bags column-wise, i.e.. a given embedding table is partitioned
along its columns and placed on specified ranks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_input_dist">
<span class="sig-name descname"><span class="pre">create_input_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><span class="pre">BaseSparseFeaturesDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_input_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_lookup">
<span class="sig-name descname"><span class="pre">create_lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor" title="torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor"><span class="pre">BaseGroupedFeatureProcessor</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup" title="torchrec.distributed.embedding_types.BaseEmbeddingLookup"><span class="pre">BaseEmbeddingLookup</span></a></span></span><a class="headerlink" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_lookup" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_output_dist">
<span class="sig-name descname"><span class="pre">create_output_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><span class="pre">BaseEmbeddingDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_output_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-torchrec.distributed.dist_data">
<span id="torchrec-distributed-dist-data"></span><h2>torchrec.distributed.dist_data<a class="headerlink" href="#module-torchrec.distributed.dist_data" title="Permalink to this heading">Â¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.EmbeddingsAllToOne">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">EmbeddingsAllToOne</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.EmbeddingsAllToOne" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Merges the pooled/sequence embedding tensor on each device into single tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<em>torch.device</em>) â€“ device on which buffer will be allocated.</p></li>
<li><p><strong>world_size</strong> (<em>int</em>) â€“ number of devices in the topology.</p></li>
<li><p><strong>cat_dim</strong> (<em>int</em>) â€“ which dimension you would like to concatenate on.
For pooled embedding it is 1; for sequence embedding it is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.EmbeddingsAllToOne.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.EmbeddingsAllToOne.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs AlltoOne operation on pooled/sequence embeddings tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tensors</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) â€“ list of embedding tensors.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of the merged embeddings.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.EmbeddingsAllToOne.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.EmbeddingsAllToOne.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTAllToAll">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">KJTAllToAll</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stagger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variable_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.KJTAllToAll" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Redistributes <cite>KeyedJaggedTensor</cite> to a <cite>ProcessGroup</cite> according to splits.</p>
<p>Implementation utilizes AlltoAll collective as part of torch.distributed.
Requires two collective calls, one to transmit final tensor lengths (to allocate
correct space), and one to transmit actual sparse values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>splits</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ List of len(pg.size()) which indicates how many features to
send to each pg.rank(). It is assumed the KeyedJaggedTensor is ordered by
destination rank. Same for all ranks.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
<li><p><strong>stagger</strong> (<em>int</em>) â€“ stagger value to apply to recat tensor, see <cite>_recat</cite> function for
more detail.</p></li>
<li><p><strong>variable_batch_size</strong> (<em>bool</em>) â€“ whether variable batch size in each rank is enabled.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span><span class="s1">&#39;B&#39;</span><span class="p">,</span><span class="s1">&#39;C&#39;</span><span class="p">]</span>
<span class="n">splits</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">kjtA2A</span> <span class="o">=</span> <span class="n">KJTAllToAll</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">splits</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">awaitable</span> <span class="o">=</span> <span class="n">kjtA2A</span><span class="p">(</span><span class="n">rank0_input</span><span class="p">)</span>

<span class="c1"># where:</span>
<span class="c1"># rank0_input is KeyedJaggedTensor holding</span>

<span class="c1">#         0           1           2</span>
<span class="c1"># &#39;A&#39;    [A.V0]       None        [A.V1, A.V2]</span>
<span class="c1"># &#39;B&#39;    None         [B.V0]      [B.V1]</span>
<span class="c1"># &#39;C&#39;    [C.V0]       [C.V1]      None</span>

<span class="c1"># rank1_input is KeyedJaggedTensor holding</span>

<span class="c1">#         0           1           2</span>
<span class="c1"># &#39;A&#39;     [A.V3]      [A.V4]      None</span>
<span class="c1"># &#39;B&#39;     None        [B.V2]      [B.V3, B.V4]</span>
<span class="c1"># &#39;C&#39;     [C.V2]      [C.V3]      None</span>

<span class="n">rank0_output</span> <span class="o">=</span> <span class="n">awaitable</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

<span class="c1"># where:</span>
<span class="c1"># rank0_output is KeyedJaggedTensor holding</span>

<span class="c1">#         0           1           2           3           4           5</span>
<span class="c1"># &#39;A&#39;     [A.V0]      None      [A.V1, A.V2]  [A.V3]      [A.V4]      None</span>
<span class="c1"># &#39;B&#39;     None        [B.V0]    [B.V1]        None        [B.V2]      [B.V3, B.V4]</span>

<span class="c1"># rank1_output is KeyedJaggedTensor holding</span>
<span class="c1">#         0           1           2           3           4           5</span>
<span class="c1"># &#39;C&#39;     [C.V0]      [C.V1]      None        [C.V2]      [C.V3]      None</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTAllToAll.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><span class="pre">KeyedJaggedTensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torchrec.distributed.dist_data.KJTAllToAllIndicesAwaitable" title="torchrec.distributed.dist_data.KJTAllToAllIndicesAwaitable"><span class="pre">KJTAllToAllIndicesAwaitable</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.KJTAllToAll.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Sends input to relevant <cite>ProcessGroup</cite> ranks.
First wait will have lengths results and issue indices/weights AlltoAll.
Second wait will have indices/weights results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><em>KeyedJaggedTensor</em></a>) â€“ input KeyedJaggedTensor of values to distribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of a KeyedJaggedTensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor">KeyedJaggedTensor</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTAllToAll.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.KJTAllToAll.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTAllToAllIndicesAwaitable">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">KJTAllToAllIndicesAwaitable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><span class="pre">KeyedJaggedTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengths</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_lengths_per_worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_lengths_per_worker</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.KJTAllToAllIndicesAwaitable" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Awaitable</span></code></a>[<a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">KeyedJaggedTensor</span></code></a>]</p>
<p>Awaitable for KJT indices and weights All2All.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>input</strong> (<a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><em>KeyedJaggedTensor</em></a>) â€“ input KJT tensor.</p></li>
<li><p><strong>lengths</strong> (<em>torch.Tensor</em>) â€“ output lengths tensor.</p></li>
<li><p><strong>splits</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ List of len(pg.size()) which indicates how many features to
send to each pg.rank(). It is assumed the <cite>KeyedJaggedTensor</cite> is ordered by
destination rank. Same for all ranks.</p></li>
<li><p><strong>keys</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) â€“ KJT keys after AlltoAll.</p></li>
<li><p><strong>recat</strong> (<em>torch.Tensor</em>) â€“ recat tensor for reordering tensor order after AlltoAll.</p></li>
<li><p><strong>in_lengths_per_worker</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of indices each rank will get.</p></li>
<li><p><strong>out_lengths_per_worker</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of indices per rank in output.</p></li>
<li><p><strong>batch_size_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ batch size per rank, need to support variable
batch size.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTAllToAllLengthsAwaitable">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">KJTAllToAllLengthsAwaitable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><span class="pre">KeyedJaggedTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stagger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variable_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.KJTAllToAllLengthsAwaitable" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Awaitable</span></code></a>[<a class="reference internal" href="#torchrec.distributed.dist_data.KJTAllToAllIndicesAwaitable" title="torchrec.distributed.dist_data.KJTAllToAllIndicesAwaitable"><code class="xref py py-class docutils literal notranslate"><span class="pre">KJTAllToAllIndicesAwaitable</span></code></a>]</p>
<p>Awaitable for KJTâ€™s lengths AlltoAll.</p>
<p>wait() waits on lengths AlltoAll, then instantiates <cite>KJTAllToAllIndicesAwaitable</cite>
awaitable where indices and weights AlltoAll will be issued.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>input</strong> (<a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><em>KeyedJaggedTensor</em></a>) â€“ input KJT tensor</p></li>
<li><p><strong>splits</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ list of len(pg.size()) which indicates how many features to
send to each pg.rank(). It is assumed the <cite>KeyedJaggedTensor</cite> is ordered by
destination rank. Same for all ranks.</p></li>
<li><p><strong>keys</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) â€“ KJT keys after AlltoAll</p></li>
<li><p><strong>stagger</strong> (<em>int</em>) â€“ stagger value to apply to recat tensor, see <cite>_recat</cite> function for
more detail.</p></li>
<li><p><strong>recat</strong> (<em>torch.Tensor</em>) â€“ recat tensor for reordering tensor order after AlltoAll.</p></li>
<li><p><strong>variable_batch_size</strong> (<em>bool</em>) â€“ whether variable batch size is enabled.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTOneToAll">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">KJTOneToAll</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.KJTOneToAll" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Redistributes <cite>KeyedJaggedTensor</cite> to all devices.</p>
<p>Implementation utilizes OnetoAll function, which essentially P2P copies the feature
to the devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>splits</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ lengths of features to split the <cite>KeyJaggedTensor</cite> features
into before copying them.</p></li>
<li><p><strong>world_size</strong> (<em>int</em>) â€“ number of devices in the topology.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTOneToAll.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kjt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><span class="pre">KeyedJaggedTensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><span class="pre">KeyedJaggedTensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.KJTOneToAll.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Splits features first and then sends the slices to the corresponding devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>kjt</strong> (<a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor"><em>KeyedJaggedTensor</em></a>) â€“ the input features.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of <cite>KeyedJaggedTensor</cite> splits.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[List[<a class="reference internal" href="torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor" title="torchrec.sparse.jagged_tensor.KeyedJaggedTensor">KeyedJaggedTensor</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.KJTOneToAll.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.KJTOneToAll.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAllGather">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">PooledEmbeddingsAllGather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">codecs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllGather" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The module class that wraps all-gather communication primitive for pooled
embedding communication</p>
<p>We have a local input tensor with a layout of
[batch_size, dimension]. We need to gather input tensors from all ranks into a flatten output tensor.</p>
<p>The class returns the async <cite>Awaitable</cite> handle for pooled embeddings tensor.
The all-gather is only available for NCCL backend.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ The process group that the all-gather communication
happens within.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">init_distributed</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="n">pg</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">PooledEmbeddingsAllGather</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAllGather.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_emb</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable" title="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable"><span class="pre">PooledEmbeddingsAwaitable</span></a></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs reduce scatter operation on pooled embeddings tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ tensor of shape [num_buckets x batch_size, dimension].</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of pooled embeddings of tensor of shape [batch_size, dimension].</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable" title="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable">PooledEmbeddingsAwaitable</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAllGather.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAllToAll">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">PooledEmbeddingsAllToAll</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_sum_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">codecs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Shards batches and collects keys of tensor with a <cite>ProcessGroup</cite> according to
<cite>dim_sum_per_rank</cite>.</p>
<p>Implementation utilizes <cite>alltoall_pooled</cite> operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>dim_sum_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of features (sum of dimensions) of the
embedding in each rank.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>Callable</em><em>[</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>torch.Tensor</em><em>]</em><em>]</em><em>]</em>) â€“ callback
functions.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dim_sum_per_rank</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">a2a</span> <span class="o">=</span> <span class="n">PooledEmbeddingsAllToAll</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">dim_sum_per_rank</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">rank0_output</span> <span class="o">=</span> <span class="n">a2a</span><span class="p">(</span><span class="n">t0</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
<span class="n">rank1_output</span> <span class="o">=</span> <span class="n">a2a</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank0_output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="c1"># torch.Size([3, 3])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rank1_output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="c1"># torch.Size([3, 3])</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.callbacks">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">callbacks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.callbacks" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable" title="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable"><span class="pre">PooledEmbeddingsAwaitable</span></a></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs AlltoAll pooled operation on pooled embeddings tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ tensor of values to distribute.</p></li>
<li><p><strong>batch_size_per_rank</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em>) â€“ batch size per rank, to support
variable batch size.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of pooled embeddings.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable" title="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable">PooledEmbeddingsAwaitable</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">PooledEmbeddingsAwaitable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor_awaitable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Awaitable</span></code></a>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Awaitable for pooled embeddings after collective operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tensor_awaitable</strong> (<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><em>Awaitable</em></a><em>[</em><em>torch.Tensor</em><em>]</em>) â€“ awaitable of concatenated tensors
from all the processes in the group after collective.</p>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable.callbacks">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">callbacks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable.callbacks" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">PooledEmbeddingsReduceScatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">codecs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The module class that wraps reduce-scatter communication primitives for pooled
embedding communication in row-wise and twrw sharding.</p>
<p>For pooled embeddings, we have a local model-parallel output tensor with a layout of
[num_buckets x batch_size, dimension]. We need to sum over num_buckets dimension
across batches. We split tensor along the first dimension into unequal chunks (tensor
slices of different buckets) according to input_splits and reduce them into the output
tensor and scatter the results for corresponding ranks.</p>
<p>The class returns the async <cite>Awaitable</cite> handle for pooled embeddings tensor.
The reduce-scatter-v is only available for NCCL backend.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ The process group that the reduce-scatter communication
happens within.</p></li>
<li><p><strong>codecs</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><em>QuantizedCommCodecs</em></a><em>]</em>) â€“ Quantization codec</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">init_distributed</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="n">pg</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">input_splits</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">PooledEmbeddingsReduceScatter</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_splits</span><span class="p">)</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable" title="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable"><span class="pre">PooledEmbeddingsAwaitable</span></a></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs reduce scatter operation on pooled embeddings tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ tensor of shape [num_buckets x batch_size, dimension].</p></li>
<li><p><strong>input_splits</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em>) â€“ list of splits for local_embs dim0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of pooled embeddings of tensor of shape [batch_size, dimension].</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable" title="torchrec.distributed.dist_data.PooledEmbeddingsAwaitable">PooledEmbeddingsAwaitable</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.SeqEmbeddingsAllToOne">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">SeqEmbeddingsAllToOne</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Merges the pooled/sequence embedding tensor on each device into single tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<em>torch.device</em>) â€“ device on which buffer will be allocated</p></li>
<li><p><strong>world_size</strong> (<em>int</em>) â€“ number of devices in the topology.</p></li>
<li><p><strong>cat_dim</strong> (<em>int</em>) â€“ which dimension you like to concate on.
For pooled embedding it is 1; for sequence embedding it is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs AlltoOne operation on pooled embeddings tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tensors</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) â€“ list of pooled embedding tensors.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of the merged pooled embeddings.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">SequenceEmbeddingsAllToAll</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">features_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">codecs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Redistributes sequence embedding to a <cite>ProcessGroup</cite> according to splits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ the process group that the AlltoAll communication
happens within.</p></li>
<li><p><strong>features_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ list of number of features per rank.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">init_distributed</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="n">pg</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="n">features_per_rank</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">SequenceEmbeddingsAllToAll</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">features_per_rank</span><span class="p">)</span>
<span class="n">local_embs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">sharding_ctx</span><span class="p">:</span> <span class="n">SequenceShardingContext</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span>
    <span class="n">local_embs</span><span class="o">=</span><span class="n">local_embs</span><span class="p">,</span>
    <span class="n">lengths</span><span class="o">=</span><span class="n">sharding_ctx</span><span class="o">.</span><span class="n">lengths_after_input_dist</span><span class="p">,</span>
    <span class="n">input_splits</span><span class="o">=</span><span class="n">sharding_ctx</span><span class="o">.</span><span class="n">input_splits</span><span class="p">,</span>
    <span class="n">output_splits</span><span class="o">=</span><span class="n">sharding_ctx</span><span class="o">.</span><span class="n">output_splits</span><span class="p">,</span>
    <span class="n">unbucketize_permute_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengths</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unbucketize_permute_tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable" title="torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable"><span class="pre">SequenceEmbeddingsAwaitable</span></a></span></span><a class="headerlink" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs AlltoAll operation on sequence embeddings tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ input embeddings tensor.</p></li>
<li><p><strong>lengths</strong> (<em>torch.Tensor</em>) â€“ lengths of sparse features after AlltoAll.</p></li>
<li><p><strong>input_splits</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ input splits of AlltoAll.</p></li>
<li><p><strong>output_splits</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ output splits of AlltoAll.</p></li>
<li><p><strong>unbucketize_permute_tensor</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) â€“ stores the permute order
of the KJT bucketize (for row-wise sharding only).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of sequence embeddings.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable" title="torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable">SequenceEmbeddingsAwaitable</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.dist_data.</span></span><span class="sig-name descname"><span class="pre">SequenceEmbeddingsAwaitable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor_awaitable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unbucketize_permute_tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Awaitable</span></code></a>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Awaitable for sequence embeddings after collective operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_awaitable</strong> (<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><em>Awaitable</em></a><em>[</em><em>torch.Tensor</em><em>]</em>) â€“ awaitable of concatenated tensors
from all the processes in the group after collective.</p></li>
<li><p><strong>unbucketize_permute_tensor</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) â€“ stores the permute order of
KJT bucketize (for row-wise sharding only).</p></li>
<li><p><strong>embedding_dim</strong> (<em>int</em>) â€“ embedding dimension.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torchrec.distributed.sharding.dp_sharding">
<span id="torchrec-distributed-sharding-dp-sharding"></span><h2>torchrec.distributed.sharding.dp_sharding<a class="headerlink" href="#module-torchrec.distributed.sharding.dp_sharding" title="Permalink to this heading">Â¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.dp_sharding.</span></span><span class="sig-name descname"><span class="pre">BaseDpEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding" title="torchrec.distributed.embedding_sharding.EmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingSharding</span></code></a>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">C</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">F</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">W</span></code>]</p>
<p>Base class for data-parallel sharding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_dims">
<span class="sig-name descname"><span class="pre">embedding_dims</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_dims" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names">
<span class="sig-name descname"><span class="pre">embedding_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names_per_rank">
<span class="sig-name descname"><span class="pre">embedding_names_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_shard_metadata">
<span class="sig-name descname"><span class="pre">embedding_shard_metadata</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardMetadata</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_shard_metadata" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.id_list_feature_names">
<span class="sig-name descname"><span class="pre">id_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.id_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.id_score_list_feature_names">
<span class="sig-name descname"><span class="pre">id_score_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.id_score_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.dp_sharding.</span></span><span class="sig-name descname"><span class="pre">DpPooledEmbeddingDist</span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEmbeddingDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Distributes pooled embeddings to be data-parallel.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharding_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>No-op as pooled embeddings are already distributed in data-parallel fashion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ output sequence embeddings.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of pooled embeddings tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.dp_sharding.</span></span><span class="sig-name descname"><span class="pre">DpPooledEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding" title="torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Shards embedding bags data-parallel, with no table sharding i.e.. a given embedding
table is replicated across all ranks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_input_dist">
<span class="sig-name descname"><span class="pre">create_input_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><span class="pre">BaseSparseFeaturesDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_input_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_lookup">
<span class="sig-name descname"><span class="pre">create_lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor" title="torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor"><span class="pre">BaseGroupedFeatureProcessor</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup" title="torchrec.distributed.embedding_types.BaseEmbeddingLookup"><span class="pre">BaseEmbeddingLookup</span></a></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_lookup" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_output_dist">
<span class="sig-name descname"><span class="pre">create_output_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><span class="pre">BaseEmbeddingDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_output_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.dp_sharding.</span></span><span class="sig-name descname"><span class="pre">DpSparseFeaturesDist</span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSparseFeaturesDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>]</p>
<p>Distributes sparse features (input) to be data-parallel.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>No-op as sparse features are already distributed in data-parallel fashion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sparse_features</strong> (<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><em>SparseFeatures</em></a>) â€“ input sparse features.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of awaitable of SparseFeatures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures">SparseFeatures</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-torchrec.distributed.sharding.rw_sharding">
<span id="torchrec-distributed-sharding-rw-sharding"></span><h2>torchrec.distributed.sharding.rw_sharding<a class="headerlink" href="#module-torchrec.distributed.sharding.rw_sharding" title="Permalink to this heading">Â¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.rw_sharding.</span></span><span class="sig-name descname"><span class="pre">BaseRwEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding" title="torchrec.distributed.embedding_sharding.EmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingSharding</span></code></a>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">C</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">F</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">W</span></code>]</p>
<p>Base class for row-wise sharding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_dims">
<span class="sig-name descname"><span class="pre">embedding_dims</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_dims" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names">
<span class="sig-name descname"><span class="pre">embedding_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names_per_rank">
<span class="sig-name descname"><span class="pre">embedding_names_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_shard_metadata">
<span class="sig-name descname"><span class="pre">embedding_shard_metadata</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardMetadata</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_shard_metadata" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.id_list_feature_names">
<span class="sig-name descname"><span class="pre">id_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.id_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.id_score_list_feature_names">
<span class="sig-name descname"><span class="pre">id_score_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.id_score_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.rw_sharding.</span></span><span class="sig-name descname"><span class="pre">RwPooledEmbeddingDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEmbeddingDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Redistributes pooled embedding tensor in RW fashion by performing a reduce-scatter
operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for reduce-scatter communication.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharding_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs reduce-scatter pooled operation on pooled embeddings tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ pooled embeddings tensor to distribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of pooled embeddings tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.rw_sharding.</span></span><span class="sig-name descname"><span class="pre">RwPooledEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding" title="torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Shards embedding bags row-wise, i.e.. a given embedding table is evenly distributed
by rows and table slices are placed on all ranks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_input_dist">
<span class="sig-name descname"><span class="pre">create_input_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><span class="pre">BaseSparseFeaturesDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_input_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_lookup">
<span class="sig-name descname"><span class="pre">create_lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor" title="torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor"><span class="pre">BaseGroupedFeatureProcessor</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup" title="torchrec.distributed.embedding_types.BaseEmbeddingLookup"><span class="pre">BaseEmbeddingLookup</span></a></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_lookup" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_output_dist">
<span class="sig-name descname"><span class="pre">create_output_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><span class="pre">BaseEmbeddingDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_output_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.rw_sharding.</span></span><span class="sig-name descname"><span class="pre">RwSparseFeaturesDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_id_list_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_id_score_list_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_list_feature_hash_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_score_list_feature_hash_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_sequence</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSparseFeaturesDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>]</p>
<p>Bucketizes sparse features in RW fashion and then redistributes with an AlltoAll
collective operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>intra_pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup within single host group for AlltoAll
communication.</p></li>
<li><p><strong>num_id_list_features</strong> (<em>int</em>) â€“ total number of id list features.</p></li>
<li><p><strong>num_id_score_list_features</strong> (<em>int</em>) â€“ total number of id score list features</p></li>
<li><p><strong>id_list_feature_hash_sizes</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ hash sizes of id list features.</p></li>
<li><p><strong>id_score_list_feature_hash_sizes</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ hash sizes of id score list
features.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
<li><p><strong>is_sequence</strong> (<em>bool</em>) â€“ if this is for a sequence embedding.</p></li>
<li><p><strong>has_feature_processor</strong> (<em>bool</em>) â€“ existence of feature processor (ie. position
weighted features).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bucketizes sparse feature values into world size number of buckets and then
performs AlltoAll operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sparse_features</strong> (<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><em>SparseFeatures</em></a>) â€“ sparse features to bucketize and
redistribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of SparseFeatures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures">SparseFeatures</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-torchrec.distributed.sharding.tw_sharding">
<span id="torchrec-distributed-sharding-tw-sharding"></span><h2>torchrec.distributed.sharding.tw_sharding<a class="headerlink" href="#module-torchrec.distributed.sharding.tw_sharding" title="Permalink to this heading">Â¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.tw_sharding.</span></span><span class="sig-name descname"><span class="pre">BaseTwEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding" title="torchrec.distributed.embedding_sharding.EmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingSharding</span></code></a>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">C</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">F</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">W</span></code>]</p>
<p>Base class for table wise sharding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_dims">
<span class="sig-name descname"><span class="pre">embedding_dims</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_dims" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names">
<span class="sig-name descname"><span class="pre">embedding_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names_per_rank">
<span class="sig-name descname"><span class="pre">embedding_names_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_shard_metadata">
<span class="sig-name descname"><span class="pre">embedding_shard_metadata</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardMetadata</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_shard_metadata" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_feature_names">
<span class="sig-name descname"><span class="pre">id_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_feature_names_per_rank">
<span class="sig-name descname"><span class="pre">id_list_feature_names_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_feature_names_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_features_per_rank">
<span class="sig-name descname"><span class="pre">id_list_features_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_features_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_feature_names">
<span class="sig-name descname"><span class="pre">id_score_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_feature_names_per_rank">
<span class="sig-name descname"><span class="pre">id_score_list_feature_names_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_feature_names_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_features_per_rank">
<span class="sig-name descname"><span class="pre">id_score_list_features_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_features_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.tw_sharding.</span></span><span class="sig-name descname"><span class="pre">InferTwEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding" title="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeaturesList" title="torchrec.distributed.embedding_types.SparseFeaturesList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeaturesList</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Shards embedding bags table-wise for inference</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_input_dist">
<span class="sig-name descname"><span class="pre">create_input_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><span class="pre">BaseSparseFeaturesDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeaturesList" title="torchrec.distributed.embedding_types.SparseFeaturesList"><span class="pre">SparseFeaturesList</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_input_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_lookup">
<span class="sig-name descname"><span class="pre">create_lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor" title="torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor"><span class="pre">BaseGroupedFeatureProcessor</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup" title="torchrec.distributed.embedding_types.BaseEmbeddingLookup"><span class="pre">BaseEmbeddingLookup</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeaturesList" title="torchrec.distributed.embedding_types.SparseFeaturesList"><span class="pre">SparseFeaturesList</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_lookup" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_output_dist">
<span class="sig-name descname"><span class="pre">create_output_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><span class="pre">BaseEmbeddingDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_output_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.tw_sharding.</span></span><span class="sig-name descname"><span class="pre">InferTwPooledEmbeddingDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEmbeddingDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Merges pooled embedding tensor from each device for inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffer will be allocated.</p></li>
<li><p><strong>world_size</strong> (<em>int</em>) â€“ number of devices in the topology.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharding_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs AlltoOne operation on pooled embedding tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>local_embs</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) â€“ pooled embedding tensors with
<cite>len(local_embs) == world_size</cite>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of merged pooled embedding tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.tw_sharding.</span></span><span class="sig-name descname"><span class="pre">InferTwSparseFeaturesDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">id_list_features_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_score_list_features_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSparseFeaturesDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeaturesList" title="torchrec.distributed.embedding_types.SparseFeaturesList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeaturesList</span></code></a>]</p>
<p>Redistributes sparse features to all devices for inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>id_list_features_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of id list features to send
to each rank.</p></li>
<li><p><strong>id_score_list_features_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of id score list features
to send to each rank.</p></li>
<li><p><strong>world_size</strong> (<em>int</em>) â€“ number of devices in the topology.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeaturesList" title="torchrec.distributed.embedding_types.SparseFeaturesList"><span class="pre">SparseFeaturesList</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs OnetoAll operation on sparse features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sparse_features</strong> (<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><em>SparseFeatures</em></a>) â€“ sparse features to redistribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of awaitable of SparseFeatures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures">SparseFeatures</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.tw_sharding.</span></span><span class="sig-name descname"><span class="pre">TwPooledEmbeddingDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_sum_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEmbeddingDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Redistributes pooled embedding tensor with an AlltoAll collective operation for
table wise sharding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>dim_sum_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of features (sum of dimensions) of the
embedding in each rank.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharding_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs AlltoAll operation on pooled embeddings tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ tensor of values to distribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of pooled embeddings.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.tw_sharding.</span></span><span class="sig-name descname"><span class="pre">TwPooledEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding" title="torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Shards embedding bags table-wise, i.e.. a given embedding table is entirely placed
on a selected rank.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_input_dist">
<span class="sig-name descname"><span class="pre">create_input_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><span class="pre">BaseSparseFeaturesDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_input_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_lookup">
<span class="sig-name descname"><span class="pre">create_lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor" title="torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor"><span class="pre">BaseGroupedFeatureProcessor</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup" title="torchrec.distributed.embedding_types.BaseEmbeddingLookup"><span class="pre">BaseEmbeddingLookup</span></a></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_lookup" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_output_dist">
<span class="sig-name descname"><span class="pre">create_output_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><span class="pre">BaseEmbeddingDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_output_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.tw_sharding.</span></span><span class="sig-name descname"><span class="pre">TwSparseFeaturesDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_list_features_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_score_list_features_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSparseFeaturesDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>]</p>
<p>Redistributes sparse features with an AlltoAll collective operation for table wise
sharding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>id_list_features_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of id list features to send to
each rank.</p></li>
<li><p><strong>id_score_list_features_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of id score list features to
send to each rank.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs AlltoAll operation on sparse features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sparse_features</strong> (<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><em>SparseFeatures</em></a>) â€“ sparse features to redistribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of awaitable of SparseFeatures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures">SparseFeatures</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-torchrec.distributed.sharding.twcw_sharding">
<span id="torchrec-distributed-sharding-twcw-sharding"></span><h2>torchrec.distributed.sharding.twcw_sharding<a class="headerlink" href="#module-torchrec.distributed.sharding.twcw_sharding" title="Permalink to this heading">Â¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twcw_sharding.TwCwPooledEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.twcw_sharding.</span></span><span class="sig-name descname"><span class="pre">TwCwPooledEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">permute_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.twcw_sharding.TwCwPooledEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding" title="torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">CwPooledEmbeddingSharding</span></code></a></p>
<p>Shards embedding bags table-wise column-wise, i.e.. a given embedding table is
partitioned along its columns and the table slices are placed on all ranks
within a host group.</p>
</dd></dl>

</section>
<section id="module-torchrec.distributed.sharding.twrw_sharding">
<span id="torchrec-distributed-sharding-twrw-sharding"></span><h2>torchrec.distributed.sharding.twrw_sharding<a class="headerlink" href="#module-torchrec.distributed.sharding.twrw_sharding" title="Permalink to this heading">Â¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.twrw_sharding.</span></span><span class="sig-name descname"><span class="pre">BaseTwRwEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding" title="torchrec.distributed.embedding_sharding.EmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingSharding</span></code></a>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">C</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">F</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">W</span></code>]</p>
<p>Base class for table wise row wise sharding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_dims">
<span class="sig-name descname"><span class="pre">embedding_dims</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_dims" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names">
<span class="sig-name descname"><span class="pre">embedding_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names_per_rank">
<span class="sig-name descname"><span class="pre">embedding_names_per_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names_per_rank" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_shard_metadata">
<span class="sig-name descname"><span class="pre">embedding_shard_metadata</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardMetadata</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_shard_metadata" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.id_list_feature_names">
<span class="sig-name descname"><span class="pre">id_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.id_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.id_score_list_feature_names">
<span class="sig-name descname"><span class="pre">id_score_list_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.id_score_list_feature_names" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.twrw_sharding.</span></span><span class="sig-name descname"><span class="pre">TwRwPooledEmbeddingDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cross_pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intra_pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_sum_per_node</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEmbeddingDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Redistributes pooled embedding tensor in TWRW fashion by performing a reduce-scatter
operation row wise on the host level and then an AlltoAll operation table wise on
the global level.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cross_pg</strong> (<em>dist.ProcessGroup</em>) â€“ global level ProcessGroup for AlltoAll
communication.</p></li>
<li><p><strong>intra_pg</strong> (<em>dist.ProcessGroup</em>) â€“ host level ProcessGroup for reduce-scatter
communication.</p></li>
<li><p><strong>dim_sum_per_node</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of features (sum of dimensions) of the
embedding for each host.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local_embs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharding_ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs reduce-scatter pooled operation on pooled embeddings tensor followed by
AlltoAll pooled operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>local_embs</strong> (<em>torch.Tensor</em>) â€“ pooled embeddings tensor to distribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of pooled embeddings tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.twrw_sharding.</span></span><span class="sig-name descname"><span class="pre">TwRwPooledEmbeddingSharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_infos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingInfo" title="torchrec.distributed.embedding_sharding.EmbeddingShardingInfo"><span class="pre">EmbeddingShardingInfo</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.ShardingEnv" title="torchrec.distributed.types.ShardingEnv"><span class="pre">ShardingEnv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qcomm_codecs_registry</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs" title="torchrec.distributed.types.QuantizedCommCodecs"><span class="pre">QuantizedCommCodecs</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding" title="torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmptyShardingContext</span></code></a>, <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
<p>Shards embedding bags table-wise then row-wise.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_input_dist">
<span class="sig-name descname"><span class="pre">create_input_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><span class="pre">BaseSparseFeaturesDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_input_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_lookup">
<span class="sig-name descname"><span class="pre">create_lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor" title="torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor"><span class="pre">BaseGroupedFeatureProcessor</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.BaseEmbeddingLookup" title="torchrec.distributed.embedding_types.BaseEmbeddingLookup"><span class="pre">BaseEmbeddingLookup</span></a></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_lookup" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_output_dist">
<span class="sig-name descname"><span class="pre">create_output_dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist" title="torchrec.distributed.embedding_sharding.BaseEmbeddingDist"><span class="pre">BaseEmbeddingDist</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmptyShardingContext" title="torchrec.distributed.embedding_sharding.EmptyShardingContext"><span class="pre">EmptyShardingContext</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_output_dist" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.sharding.twrw_sharding.</span></span><span class="sig-name descname"><span class="pre">TwRwSparseFeaturesDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intra_pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_list_features_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_score_list_features_per_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_list_feature_hash_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">id_score_list_feature_hash_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_feature_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist" title="torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSparseFeaturesDist</span></code></a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseFeatures</span></code></a>]</p>
<p>Bucketizes sparse features in TWRW fashion and then redistributes with an AlltoAll
collective operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup for AlltoAll communication.</p></li>
<li><p><strong>intra_pg</strong> (<em>dist.ProcessGroup</em>) â€“ ProcessGroup within single host group for AlltoAll
communication.</p></li>
<li><p><strong>id_list_features_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of id list features to send to
each rank.</p></li>
<li><p><strong>id_score_list_features_per_rank</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ number of id score list features to
send to each rank.</p></li>
<li><p><strong>id_list_feature_hash_sizes</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ hash sizes of id list features.</p></li>
<li><p><strong>id_score_list_feature_hash_sizes</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) â€“ hash sizes of id score list
features.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>torch.device</em><em>]</em>) â€“ device on which buffers will be allocated.</p></li>
<li><p><strong>has_feature_processor</strong> (<em>bool</em>) â€“ existence of a feature processor (ie. position
weighted features).</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">3</span> <span class="n">features</span>
<span class="mi">2</span> <span class="n">hosts</span> <span class="k">with</span> <span class="mi">2</span> <span class="n">devices</span> <span class="n">each</span>

<span class="n">Bucketize</span> <span class="n">each</span> <span class="n">feature</span> <span class="n">into</span> <span class="mi">2</span> <span class="n">buckets</span>
<span class="n">Staggered</span> <span class="n">shuffle</span> <span class="k">with</span> <span class="n">feature</span> <span class="n">splits</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">AlltoAll</span> <span class="n">operation</span>

<span class="n">NOTE</span><span class="p">:</span> <span class="n">result</span> <span class="n">of</span> <span class="n">staggered</span> <span class="n">shuffle</span> <span class="ow">and</span> <span class="n">AlltoAll</span> <span class="n">operation</span> <span class="n">look</span> <span class="n">the</span> <span class="n">same</span> <span class="n">after</span>
<span class="n">reordering</span> <span class="ow">in</span> <span class="n">AlltoAll</span>

<span class="n">Result</span><span class="p">:</span>
    <span class="n">host</span> <span class="mi">0</span> <span class="n">device</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">feature</span> <span class="mi">0</span> <span class="n">bucket</span> <span class="mi">0</span>
        <span class="n">feature</span> <span class="mi">1</span> <span class="n">bucket</span> <span class="mi">0</span>

    <span class="n">host</span> <span class="mi">0</span> <span class="n">device</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">feature</span> <span class="mi">0</span> <span class="n">bucket</span> <span class="mi">1</span>
        <span class="n">feature</span> <span class="mi">1</span> <span class="n">bucket</span> <span class="mi">1</span>

    <span class="n">host</span> <span class="mi">1</span> <span class="n">device</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">feature</span> <span class="mi">2</span> <span class="n">bucket</span> <span class="mi">0</span>

    <span class="n">host</span> <span class="mi">1</span> <span class="n">device</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">feature</span> <span class="mi">2</span> <span class="n">bucket</span> <span class="mi">1</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable"><span class="pre">Awaitable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><span class="pre">SparseFeatures</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bucketizes sparse feature values into local world size number of buckets,
performs staggered shuffle on the sparse features, and then performs AlltoAll
operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sparse_features</strong> (<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures"><em>SparseFeatures</em></a>) â€“ sparse features to bucketize and
redistribute.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>awaitable of SparseFeatures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.types.Awaitable" title="torchrec.distributed.types.Awaitable">Awaitable</a>[<a class="reference internal" href="torchrec.distributed.html#torchrec.distributed.embedding_types.SparseFeatures" title="torchrec.distributed.embedding_types.SparseFeatures">SparseFeatures</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.training" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torchrec.fx.html" class="btn btn-neutral float-right" title="torchrec.fx" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="torchrec.distributed.planner.html" class="btn btn-neutral" title="torchrec.distributed.planner" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torchrec.distributed.sharding</a><ul>
<li><a class="reference internal" href="#module-torchrec.distributed.sharding.cw_sharding">torchrec.distributed.sharding.cw_sharding</a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">BaseCwEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_dims"><code class="docutils literal notranslate"><span class="pre">BaseCwEmbeddingSharding.embedding_dims()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding.embedding_names"><code class="docutils literal notranslate"><span class="pre">BaseCwEmbeddingSharding.embedding_names()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">CwPooledEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_input_dist"><code class="docutils literal notranslate"><span class="pre">CwPooledEmbeddingSharding.create_input_dist()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_lookup"><code class="docutils literal notranslate"><span class="pre">CwPooledEmbeddingSharding.create_lookup()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding.create_output_dist"><code class="docutils literal notranslate"><span class="pre">CwPooledEmbeddingSharding.create_output_dist()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-torchrec.distributed.dist_data">torchrec.distributed.dist_data</a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.EmbeddingsAllToOne"><code class="docutils literal notranslate"><span class="pre">EmbeddingsAllToOne</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.EmbeddingsAllToOne.forward"><code class="docutils literal notranslate"><span class="pre">EmbeddingsAllToOne.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.EmbeddingsAllToOne.training"><code class="docutils literal notranslate"><span class="pre">EmbeddingsAllToOne.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTAllToAll"><code class="docutils literal notranslate"><span class="pre">KJTAllToAll</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTAllToAll.forward"><code class="docutils literal notranslate"><span class="pre">KJTAllToAll.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTAllToAll.training"><code class="docutils literal notranslate"><span class="pre">KJTAllToAll.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTAllToAllIndicesAwaitable"><code class="docutils literal notranslate"><span class="pre">KJTAllToAllIndicesAwaitable</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTAllToAllLengthsAwaitable"><code class="docutils literal notranslate"><span class="pre">KJTAllToAllLengthsAwaitable</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTOneToAll"><code class="docutils literal notranslate"><span class="pre">KJTOneToAll</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTOneToAll.forward"><code class="docutils literal notranslate"><span class="pre">KJTOneToAll.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.KJTOneToAll.training"><code class="docutils literal notranslate"><span class="pre">KJTOneToAll.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllGather"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAllGather</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.forward"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAllGather.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllGather.training"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAllGather.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAllToAll</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.callbacks"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAllToAll.callbacks</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.forward"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAllToAll.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAllToAll.training"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAllToAll.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAwaitable</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable.callbacks"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsAwaitable.callbacks</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsReduceScatter</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.forward"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsReduceScatter.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter.training"><code class="docutils literal notranslate"><span class="pre">PooledEmbeddingsReduceScatter.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne"><code class="docutils literal notranslate"><span class="pre">SeqEmbeddingsAllToOne</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.forward"><code class="docutils literal notranslate"><span class="pre">SeqEmbeddingsAllToOne.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.SeqEmbeddingsAllToOne.training"><code class="docutils literal notranslate"><span class="pre">SeqEmbeddingsAllToOne.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll"><code class="docutils literal notranslate"><span class="pre">SequenceEmbeddingsAllToAll</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.forward"><code class="docutils literal notranslate"><span class="pre">SequenceEmbeddingsAllToAll.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll.training"><code class="docutils literal notranslate"><span class="pre">SequenceEmbeddingsAllToAll.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable"><code class="docutils literal notranslate"><span class="pre">SequenceEmbeddingsAwaitable</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torchrec.distributed.sharding.dp_sharding">torchrec.distributed.sharding.dp_sharding</a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_dims"><code class="docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding.embedding_dims()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names"><code class="docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding.embedding_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_names_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding.embedding_names_per_rank()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.embedding_shard_metadata"><code class="docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding.embedding_shard_metadata()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.id_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding.id_list_feature_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding.id_score_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseDpEmbeddingSharding.id_score_list_feature_names()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist"><code class="docutils literal notranslate"><span class="pre">DpPooledEmbeddingDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.forward"><code class="docutils literal notranslate"><span class="pre">DpPooledEmbeddingDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist.training"><code class="docutils literal notranslate"><span class="pre">DpPooledEmbeddingDist.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">DpPooledEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_input_dist"><code class="docutils literal notranslate"><span class="pre">DpPooledEmbeddingSharding.create_input_dist()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_lookup"><code class="docutils literal notranslate"><span class="pre">DpPooledEmbeddingSharding.create_lookup()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding.create_output_dist"><code class="docutils literal notranslate"><span class="pre">DpPooledEmbeddingSharding.create_output_dist()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist"><code class="docutils literal notranslate"><span class="pre">DpSparseFeaturesDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.forward"><code class="docutils literal notranslate"><span class="pre">DpSparseFeaturesDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist.training"><code class="docutils literal notranslate"><span class="pre">DpSparseFeaturesDist.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-torchrec.distributed.sharding.rw_sharding">torchrec.distributed.sharding.rw_sharding</a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_dims"><code class="docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding.embedding_dims()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names"><code class="docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding.embedding_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_names_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding.embedding_names_per_rank()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.embedding_shard_metadata"><code class="docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding.embedding_shard_metadata()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.id_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding.id_list_feature_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding.id_score_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseRwEmbeddingSharding.id_score_list_feature_names()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist"><code class="docutils literal notranslate"><span class="pre">RwPooledEmbeddingDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.forward"><code class="docutils literal notranslate"><span class="pre">RwPooledEmbeddingDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist.training"><code class="docutils literal notranslate"><span class="pre">RwPooledEmbeddingDist.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">RwPooledEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_input_dist"><code class="docutils literal notranslate"><span class="pre">RwPooledEmbeddingSharding.create_input_dist()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_lookup"><code class="docutils literal notranslate"><span class="pre">RwPooledEmbeddingSharding.create_lookup()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding.create_output_dist"><code class="docutils literal notranslate"><span class="pre">RwPooledEmbeddingSharding.create_output_dist()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist"><code class="docutils literal notranslate"><span class="pre">RwSparseFeaturesDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.forward"><code class="docutils literal notranslate"><span class="pre">RwSparseFeaturesDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist.training"><code class="docutils literal notranslate"><span class="pre">RwSparseFeaturesDist.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-torchrec.distributed.sharding.tw_sharding">torchrec.distributed.sharding.tw_sharding</a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_dims"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.embedding_dims()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.embedding_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_names_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.embedding_names_per_rank()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.embedding_shard_metadata"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.embedding_shard_metadata()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.id_list_feature_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_feature_names_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.id_list_feature_names_per_rank()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_list_features_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.id_list_features_per_rank()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.id_score_list_feature_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_feature_names_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.id_score_list_feature_names_per_rank()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding.id_score_list_features_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseTwEmbeddingSharding.id_score_list_features_per_rank()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">InferTwEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_input_dist"><code class="docutils literal notranslate"><span class="pre">InferTwEmbeddingSharding.create_input_dist()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_lookup"><code class="docutils literal notranslate"><span class="pre">InferTwEmbeddingSharding.create_lookup()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding.create_output_dist"><code class="docutils literal notranslate"><span class="pre">InferTwEmbeddingSharding.create_output_dist()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist"><code class="docutils literal notranslate"><span class="pre">InferTwPooledEmbeddingDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.forward"><code class="docutils literal notranslate"><span class="pre">InferTwPooledEmbeddingDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist.training"><code class="docutils literal notranslate"><span class="pre">InferTwPooledEmbeddingDist.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist"><code class="docutils literal notranslate"><span class="pre">InferTwSparseFeaturesDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.forward"><code class="docutils literal notranslate"><span class="pre">InferTwSparseFeaturesDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist.training"><code class="docutils literal notranslate"><span class="pre">InferTwSparseFeaturesDist.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist"><code class="docutils literal notranslate"><span class="pre">TwPooledEmbeddingDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.forward"><code class="docutils literal notranslate"><span class="pre">TwPooledEmbeddingDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist.training"><code class="docutils literal notranslate"><span class="pre">TwPooledEmbeddingDist.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">TwPooledEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_input_dist"><code class="docutils literal notranslate"><span class="pre">TwPooledEmbeddingSharding.create_input_dist()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_lookup"><code class="docutils literal notranslate"><span class="pre">TwPooledEmbeddingSharding.create_lookup()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding.create_output_dist"><code class="docutils literal notranslate"><span class="pre">TwPooledEmbeddingSharding.create_output_dist()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist"><code class="docutils literal notranslate"><span class="pre">TwSparseFeaturesDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.forward"><code class="docutils literal notranslate"><span class="pre">TwSparseFeaturesDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist.training"><code class="docutils literal notranslate"><span class="pre">TwSparseFeaturesDist.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-torchrec.distributed.sharding.twcw_sharding">torchrec.distributed.sharding.twcw_sharding</a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twcw_sharding.TwCwPooledEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">TwCwPooledEmbeddingSharding</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torchrec.distributed.sharding.twrw_sharding">torchrec.distributed.sharding.twrw_sharding</a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_dims"><code class="docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding.embedding_dims()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names"><code class="docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding.embedding_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_names_per_rank"><code class="docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding.embedding_names_per_rank()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.embedding_shard_metadata"><code class="docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding.embedding_shard_metadata()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.id_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding.id_list_feature_names()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding.id_score_list_feature_names"><code class="docutils literal notranslate"><span class="pre">BaseTwRwEmbeddingSharding.id_score_list_feature_names()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist"><code class="docutils literal notranslate"><span class="pre">TwRwPooledEmbeddingDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.forward"><code class="docutils literal notranslate"><span class="pre">TwRwPooledEmbeddingDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist.training"><code class="docutils literal notranslate"><span class="pre">TwRwPooledEmbeddingDist.training</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding"><code class="docutils literal notranslate"><span class="pre">TwRwPooledEmbeddingSharding</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_input_dist"><code class="docutils literal notranslate"><span class="pre">TwRwPooledEmbeddingSharding.create_input_dist()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_lookup"><code class="docutils literal notranslate"><span class="pre">TwRwPooledEmbeddingSharding.create_lookup()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding.create_output_dist"><code class="docutils literal notranslate"><span class="pre">TwRwPooledEmbeddingSharding.create_output_dist()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist"><code class="docutils literal notranslate"><span class="pre">TwRwSparseFeaturesDist</span></code></a><ul>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.forward"><code class="docutils literal notranslate"><span class="pre">TwRwSparseFeaturesDist.forward()</span></code></a></li>
<li><a class="reference internal" href="#torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist.training"><code class="docutils literal notranslate"><span class="pre">TwRwSparseFeaturesDist.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>